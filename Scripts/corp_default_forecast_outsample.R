# File: corp_default_forecast_outsample.R
# Authors: Tyler Pike
# Date: 
# Note(s): Perform out-of-sample forecasting exercises 
#          Users will need access to FRB serves to access data, but may substitute public sources as desired

# clear enviroment
rm(list = ls())

# load libraries
library(tis)
library(stfm.helper, lib.loc="/stfm/shared1/R")
library(dplyr)
library(lubridate)

##################################################################################
# Miscellanious utility functions
##################################################################################
# function to calc the dim of an object no matter the type
DIM = function(X){if(is.null(dim(X))) return(length(X)); return(dim(X))}

# function to standardize vector 
standardize = function(X){(X-mean(X, na.rm = T))/sd(X, na.rm = T)}

# function to calc root mean squared error (takes in vector of residuals)
RMSE = function(X){return(sqrt(mean(X^2)))}

# function to create percent dif of a series 
percentDif = function(X,n){return((X-dplyr::lag(X,n=n))/dplyr::lag(X,n=n))}

# function to create rho terms for psuedo-AR
rhoCreation = function(X, d, l = 1){
  X = as.matrix(X)
  colnames(X) = 'x'
  X = as.data.frame(X) %>% mutate(dx = x - dplyr::lag(x, 1),
                                  x.l0  = dx,
                                  x.l1  = dplyr::lag(dx,n=1),
                                  x.l2  = dplyr::lag(dx,n=2),
                                  x.l3  = dplyr::lag(dx,n=3),
                                  x.l4  = dplyr::lag(dx,n=4),
                                  x.l5  = dplyr::lag(dx,n=5)) %>% select(-x, -dx)
  
  X = X[2:d] 
  
  return(X)
}

# function to convert daily series to monthly 
# method takes in either 'mean,' which calculates each monthly value as the 
#   mean of the higher frequency observations within that month, or 
#   'LOM', which assigns each month the last observation value of that month
# takes in matrix with a 'date' column and a numeric value column
convert2Monthly = function(X, method = 'mean'){
  # preserve original index name
  origName = colnames(X) 
  origName = origName[origName != 'date']
  # group into months
  X = X %>% dplyr::rename(index = colnames(X)[colnames(X) != 'date']) %>% 
    group_by(month = month(date), year = year(date))
  # convert by chosen method
  if(method == 'mean'){
    X = X  %>% summarize(origName = mean(index, na.rm = T)) 
  } else if(method == 'LOM'){
    X = X  %>% filter(date == max(date)) %>% 
      summarize(origName = index) 
  } else{
    print('Please enter a valid method of monthly conversion')
  }
  # ungroup and clean up
  X = X %>% mutate(date = ymd(paste0(year,'-',month,'-01'))) %>%
    ungroup() %>% select(-month, -year) %>% arrange(date) 
  # reassign original name 
  colnames(X)[colnames(X) != 'date'] <- origName
  # return converted data frame
  return(X)
}

convert2Quarterly = function(X, method = 'mean'){
  #preserve original index name
  origName = colnames(X) 
  origName = origName[origName != 'date']
  #group into months
  X = X %>% dplyr::rename(index = colnames(X)[colnames(X) != 'date']) %>% 
    group_by(month = quarter(date)*3, year = year(date))
  #convert by chosen method
  if(method == 'mean'){
    X = X  %>% summarize(origName = mean(index, na.rm = T)) 
  } else if(method == 'LOM'){
    X = X  %>% filter(date == max(date)) %>% 
      summarize(origName = index) 
  } else{
    print('Please enter a valid method of monthly conversion')
  }
  #ungroup and clean up
  X = X %>% mutate(date = ymd(paste0(year,'-',month,'-01'))) %>%
    ungroup() %>% select(-month, -year) %>% arrange(date) 
  #reassign original name 
  colnames(X)[colnames(X) != 'date'] <- origName
  #return converted data frame
  return(X)
}

# CW test for comparitive preformance of nested models
#  y is the observed value being predicted
#  y1 is the prediction generated by the parsimonious model 
#  y2 is the prediction generated by the augmented model
#  h is forecast horizon 
cw.test = function(y,y1,y2,h){
  fCW12 = (y - y1)^2 - ((y-y2)^2 - (y1 - y2)^2)
  lmCW <- lm(as.numeric(fCW12)~1)
  lmCW.summ <- summary(lmCW)
  lmCW.NW.summ <- lmCW.summ
  lmCW.NW.summ$coefficients <- unclass(lmtest::coeftest(lmCW, vcov. = sandwich::NeweyWest(lmCW,lag=(h-1))))
  MSPEadj12 <- lmCW.NW.summ$coefficients[3]
  
  return(MSPEadj12)
}

##################################################################################
# Functin to download the raw data to work with 
##################################################################################
setUpData = function(select_Y, select_X, h, log = T){
  
  #----------------------
  # import and clean X
  #----------------------
  if(select_X == 'Chicago'){
    quantmod::getSymbols('NFCI', src = 'FRED', env = globalenv())
    rawreg = data.frame(date = index(NFCI), NFCI = coredata(NFCI))  %>% convert2Quarterly()
  }else if(select_X == 'StLouis'){
    quantmod::getSymbols('STLFSI', src = 'FRED', env = globalenv())
    rawreg = data.frame(date = index(STLFSI), STLFSI = coredata(STLFSI))  %>% convert2Quarterly()
  }else if(select_X == 'Kansas'){
    quantmod::getSymbols('KCFSI', src = 'FRED', env = globalenv())
    rawreg = data.frame(date = index(KCFSI), KCFSI = coredata(KCFSI))  %>% convert2Quarterly()
  }else if(select_X == 'TS'){
    quantmod::getSymbols('T10Y3M', src = 'FRED', env = globalenv())
    rawreg = data.frame(date = index(T10Y3M), T10Y3M = coredata(T10Y3M))  %>% convert2Quarterly()
  }else if(select_X == 'CORP'){
    quantmod::getSymbols('BAA10Y', src = 'FRED', env = globalenv())
    rawreg = data.frame(date = index(BAA10Y), BAA10Y = coredata(BAA10Y))  %>% convert2Quarterly()
  }else if(select_X == 'GS'){
    rawreg =  as.data.frame(getfame_dt('GSUSFCI.INDEX','spx_futures')) %>% convert2Quarterly()
  }else if(select_X == 'EBP'){
    rawreg = read.csv('./Data/Input/ebp_m.csv', stringsAsFactors = F) %>%
      mutate(date = as_date(date)) %>% select(date, ebp) %>% convert2Quarterly()
  }else if(select_X == 'NFCH'){
    rawreg = read.csv('./Data/Ouput/indexes_aggregate.csv', stringsAsFactors = F) %>%
      mutate(date = ymd(date)) %>% select(date, NFCH) %>% convert2Quarterly()
  }
  
  #----------------------
  # import and clean Y
  #----------------------
  # load in unemployment and gdp 
  usTickers = c('gdp_xcw_09.q','ruc.q')
  quarterlyData = 
    getfame_dt(usTickers,"us") %>% 
    rename(gdp = gdp_xcw_09.q, urate = ruc.q) 
  day(quarterlyData$date) <- 1
  
  # load in Inudstrial Production and payroll employment
  monthlyData = 
    getfame_dt(c("JQI.M", "EE.M", "CE_XCW_09.M"), "us") %>% 
    group_by(year = year(date), quarter = quarter(date)) %>% 
    summarise(ip = mean(JQI.M, na.rm = T),
              payroll = mean(EE.M, na.rm = T),
              consumption = mean(CE_XCW_09.M),
              date = max(date)) %>% 
    ungroup() 
  day(monthlyData$date) = 1
  
  #strip out unwanted Y's and convert to quarterly
  rawtar = 
    full_join(quarterlyData, monthlyData, by = 'date') %>%
    select(date, select_Y) %>%
    na.omit()
  
  #take log of Y
  if(log == T){
    if(select_Y == 'gdp' | select_Y == 'ip' | select_Y == 'payroll' | select_Y == 'consumption'){
      rawtar[,c(select_Y)] = log(rawtar[,c(select_Y)])
    }
  }
  
  #----------------------
  # merge and return
  #----------------------
  masterData = 
    merge(rawtar, rawreg, by = 'date', all = T) %>%
    tidyr::fill(everything())
  return(masterData)
}

##################################################################################
# Forecast exercise function
##################################################################################
NYForecastExercise = function(select_X,                     #which X matrix to use
                              select_Y,                     #which Y matrix to use
                              h = 6,                        #forecast horizon 
                              PCA = F){                     #binary to create PCA index    
                            
                              
  # import and create the origional data set of variables
  masterData = setUpData(select_Y, select_X, h) %>% na.omit()
  
  # set the first date to forecast from, if data does not
  # go back to 1985, then use as much data as possible
  if(as.Date('1989-01-01') %in% masterData$date){
    startDate = as.Date('1996-03-01')
  }else{
    startDate = masterData$date[1]
    year(startDate) = year(startDate) + 7
  }
  startIndex = which(masterData$date == as.Date(startDate))
  
  # #10 years of data
  # if(as.Date('1973-01-01') %in% masterData$date){
  #   startDate = as.Date('1985-03-01')
  # }else{
  #   startDate = masterData$date[1]
  #   year(startDate) = year(startDate) + 12
  # }
  # startIndex = which(masterData$date == as.Date(startDate))
  
  # instantiate empty vectors for items of interest
  Predicted_FCI = vector(length = nrow(masterData))
  Predicted_AR = vector(length = nrow(masterData))
  Predicted_RW= vector(length = nrow(masterData))
  Date = vector(length = nrow(masterData))

  for(iteration in startIndex:nrow(masterData)){
  
    #-------------------------------
    # Generate Y hats for PLS 
    #-------------------------------
    #create the X and Y matrices (through time t)
    rawData = masterData[1:iteration,]
    
    #transform Y appropriatley 
    if(select_Y != 'urate' & select_Y != 'cfnai'){
      rawData[,c(select_Y)] = log(rawData[,select_Y])
    }
    
    #create PCA index if chosen 
    #if(select_X == 'Hybrid'){PCA = T}
    if(select_X == 'Hybrid'){
      pcaIndex = rawData %>% select(-date, -select_Y) %>% as.matrix()
      pcaIndex = apply(pcaIndex, FUN = standardize, MARGIN = 2)
      pcaIndex = pcaIndex %*% prcomp(pcaIndex)$rotation[,1]
      rawData = data.frame(rawData, pcaIndex) %>% select(date, select_Y, pcaIndex)
    }
    
    #create design matrix (X)
    fci = rawData %>% select( -select_Y) 
    y = rawData %>% select(select_Y) #keep in levels
    regressors = data.frame(intercept = 1,fci,rhoCreation(y,5)) 

    #properly subset and difference Y 
    y = rawData %>% 
        select(target = select_Y, date)  %>%
        mutate(Y = dplyr::lead(target, n = h) - target) 

    #merge Y and X to guarentee aligned dates 
    XY = dplyr::inner_join(regressors,y, by = 'date') %>% na.omit() %>% select(-date)
    X = XY %>% select(-target,-Y) %>% as.matrix()
    Y = XY %>% select(Y) %>% as.matrix()

    #calculate forecasting betas
    beta = solve(t(X)%*%X)%*%t(X)%*%Y
  
    #predict h steps ahead value 
    extension = regressors %>% select(-date) %>% tail(1) %>% as.matrix()
    Yhat_FCI = extension %*% beta
    
    #-------------------------------
    # Generate Y hats for base AR
    #-------------------------------
    #create design matrix (X)
    y = rawData %>% select(select_Y) 
    regressors = data.frame(intercept = 1,rhoCreation(y,5), date = rawData$date) 
    
    #properly subset and difference Y 
    y = rawData %>% 
      select(target = select_Y, date)  %>%
      mutate(Y = dplyr::lead(target, n = h) - target) %>% na.omit()
    
    #merge Y and X to guarentee aligned dates 
    XY = dplyr::inner_join(regressors,y, by = 'date') %>% na.omit()
    X = XY %>% select(-date,-target,-Y) %>% as.matrix()
    Y = XY %>% select(Y) %>% as.matrix()
    
    #calculate forecasting betas
    beta = solve(t(X)%*%X)%*%t(X)%*%Y
    
    #predict h steps ahead value 
    extension = regressors %>% 
      select(-date) %>% 
      tail(1) %>% a
      s.matrix()
    
    Yhat_AR = extension %*% beta
    
    #-----------------------------------
    # Generate Y hats for Random Walk
    #-----------------------------------
    #Y matrix from AR fit 
    Yhat_RW = tail(Y,1)
  
    #prepare vectors for export
    Predicted_FCI[iteration] = Yhat_FCI
    Predicted_AR[iteration] = Yhat_AR
    Predicted_RW[iteration] = Yhat_RW
    Date[iteration] = masterData$date[iteration]
    
  }  

  #-------------------------
  # Final analysis
  #-------------------------
  results = 
    data.frame(date = as.Date(Date),
                Predicted_FCI,
                Predicted_AR,
                Predicted_RW) %>% 
    filter(date >= startDate)

  actual = masterData %>% 
    select(date, select_Y) %>% 
    dplyr::rename(target = select_Y) 
  
  if(select_Y != 'urate' & select_Y != 'cfnai'){actual$target = log(actual$target)}

  actual = actual %>% 
    mutate(target = (dplyr::lead(target, n = h) - target)) %>%
    filter(date >= as.Date(startDate))

  info = merge(results, actual, by = 'date', all=T) %>% 
    mutate(resid_fci = - target + Predicted_FCI,
            resid_ar  = - target + Predicted_AR,
            resid_rw  = - target + Predicted_RW) %>% na.omit()
 
  return(info)
  
}

##################################################################################
#            Wrapper to Run Analysis in Parallel
##################################################################################
parFunction = function(x,y,h){
  
  forecasts = 
    NYForecastExercise(select_X = x,
                      select_Y = y,
                      h = h)
  
  test.ar.dm = 
    forecast::dm.test(
      (forecasts$resid_ar),
      (forecasts$resid_fci),
      alternative = 'greater',
      h = h-1,
      power = 2)
  
  test.rw.dm = 
    forecast::dm.test(
      (forecasts$resid_rw),
      (forecasts$resid_fci),
      alternative = 'greater',
      h = h-1,
      power = 2)

  test.ar.cw = 
    cw.test(y = forecasts$target,
            y1 = forecasts$Predicted_AR,
            y2 = forecasts$Predicted_FCI,
            h)

  searchInfo = 
    data.frame(
      y,
      x,
      h,
      RW_Ratio = RMSE(forecasts$resid_fci)/RMSE(forecasts$resid_rw),
      RW_DM = test.rw.dm$p.value,
      AR_Ratio = RMSE(forecasts$resid_fci)/RMSE(forecasts$resid_ar),
      AR_DM = test.ar.dm$p.value,
      AR_CW_lvl = test.ar.cw
    )

  searchInfo = searchInfo %>%
    mutate(AR_CW_Sig = if_else(AR_CW_lvl >= 1.282, 10, as.numeric(NA)),
          AR_CW_Sig = if_else(AR_CW_lvl >= 1.645, 5, AR_CW_Sig))

  return(searchInfo)
  
}

##################################################################################
#            Run the Out of Sample tests (in parallel)
##################################################################################
# X's: MFA, Kansas, Chicago, StLouis, GS, TS, CORP, EBP, NFCD, BSCI
# Y's: payroll, urate, ip, consumption, retailSales, cfnai

#set up parallel machinary
library(doParallel)
library(foreach)
doParallel::registerDoParallel(20)

H = c(4)
X = c('MFA', 'Kansas', 'Chicago', 'StLouis', 'GS', 'TS', 'CORP','EBP','NFCD')
Y = c('payroll', 'urate', 'ip', 'gdp')

start = Sys.time()
searchInfo = foreach( i= 1:length(Y), .combine = rbind) %:%
                foreach( k= 1:length(X), .combine = rbind) %:%
                   foreach( j= 1:length(H), .combine = rbind) %dopar%
                      parFunction(x = X[k], y = Y[i], h = H[j])

print(Sys.time() - start)

stopImplicitCluster()

# save results
write.csv(searchInfo, file = './Evaluation/Forecsat/outsample_forecasts_results.csv', row.names = F)
